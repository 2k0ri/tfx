{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiTx9KF4O6gt"
      },
      "source": [
        "##### Copyright \u0026copy; 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goQ0jLHAeMek"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6x1ypzczQCwy"
      },
      "source": [
        "# TFX Pipeline Tutorial using Penguin dataset\n",
        "\n",
        "***A Short tutorial to run a simple TFX pipeline.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIEpi1qCQJ8r"
      },
      "source": [
        "Note: We recommend running this tutorial in a Colab notebook, with no setup required!  Just click \"Run in Google Colab\".\n",
        "\n",
        "\u003cdiv class=\"devsite-table-wrapper\"\u003e\u003ctable class=\"tfo-notebook-buttons\" align=\"left\"\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://www.tensorflow.org/tfx/tutorials/tfx/penguin_simple\"\u003e\n",
        "\u003cimg src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" /\u003eView on TensorFlow.org\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tfx/blob/master/docs/tutorials/tfx/penguin_simple.ipynb\"\u003e\n",
        "\u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\"\u003eRun in Google Colab\u003c/a\u003e\u003c/td\u003e\n",
        "\u003ctd\u003e\u003ca target=\"_blank\" href=\"https://github.com/tensorflow/tfx/tree/master/docs/tutorials/tfx/penguin_simple.ipynb\"\u003e\n",
        "\u003cimg width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\"\u003eView source on GitHub\u003c/a\u003e\u003c/td\u003e\n",
        "\u003c/table\u003e\u003c/div\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VuwrlnvQJ5k"
      },
      "source": [
        "This Colab-based tutorial will create and run a TFX pipeline. The pipeline will consist of three essential components, ExampleGen, Trainer and Pusher. Please refer other tutorials like [Component tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras) to learn full-capability of TFX."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4U5gp15QJ2b"
      },
      "source": [
        "## Setup\n",
        "First, we install and import the necessary packages, set up paths, and download data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVhDI9dzQJzp"
      },
      "source": [
        "### Upgrade Pip\n",
        "\n",
        "To avoid upgrading Pip in a system when running locally, check to make sure that we're running in Colab.  Local systems can of course be upgraded separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fW2ytRrXR4C2"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import colab\n",
        "  !pip install --upgrade pip\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twV49r0aQJvq"
      },
      "source": [
        "### Install TFX\n",
        "\n",
        "**Note: In Google Colab, because of package updates, the first time you run this cell you must restart the runtime (Runtime \u003e Restart runtime ...).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmuwJKNRR_fw"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U --use-feature=2020-resolver --pre tfx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKKR6fuwQJq1"
      },
      "source": [
        "#### Did you restart the runtime?\n",
        "\n",
        "If you are using Google Colab, the first time that you run the cell above, you must restart the runtime (Runtime \u003e Restart runtime ...). This is because of the way that Colab loads packages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMvY087-QJfH"
      },
      "source": [
        "### Import packages\n",
        "We import necessary packages, including standard TFX component classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00StSsGHP8M2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib\n",
        "\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "tf.get_logger().propagate = False\n",
        "\n",
        "import tfx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VHG7XszSO0g"
      },
      "source": [
        "Let's check the library versions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsDGBQSxSLEh"
      },
      "outputs": [],
      "source": [
        "print('TensorFlow version: {}'.format(tf.__version__))\n",
        "print('TFX version: {}'.format(tfx.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CryTzJkMSZD9"
      },
      "source": [
        "### Set up pipeline paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UJ_g8HESaRs"
      },
      "outputs": [],
      "source": [
        "_pipeline_name = 'penguin_simple'\n",
        "\n",
        "# Path to various pipeline artifact.\n",
        "_pipeline_root = os.path.join('pipelines', _pipeline_name)\n",
        "# Path to ML metadata Sqlite db.\n",
        "_metadata_path = os.path.join('metadata', _pipeline_name,\n",
        "                              'metadata.db')\n",
        "\n",
        "# This is the path where your model will be pushed for serving.\n",
        "_serving_model_dir = os.path.join('serving_model', _pipeline_name)\n",
        "\n",
        "# Set up logging.\n",
        "logging.set_verbosity(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8F2SRwRLSYGa"
      },
      "source": [
        "### Download example data\n",
        "We download the example dataset for use in our TFX pipeline. The dataset we're using is [Palmer Penguins dataset](https://allisonhorst.github.io/palmerpenguins/articles/intro.html) which is also used in the [examples](https://github.com/tensorflow/tfx/tree/master/tfx/examples/penguin). \n",
        "\n",
        "There are four numeric features in this dataset:\n",
        "\n",
        "- culmen_length_mm\n",
        "- culmen_depth_mm\n",
        "- flipper_length_mm\n",
        "- body_mass_g\n",
        "\n",
        "All features were already normalized to 0~1. We will build a classification model which predicts the `species` of penguins.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRmwdV_DSjcy"
      },
      "outputs": [],
      "source": [
        "_data_root = os.path.join('data', _pipeline_name)\n",
        "os.makedirs(_data_root)\n",
        "DATA_PATH = 'https://raw.githubusercontent.com/tensorflow/tfx/master/tfx/examples/penguin/data/penguins_processed.csv'\n",
        "_data_filepath = os.path.join(_data_root, \"data.csv\")\n",
        "urllib.request.urlretrieve(DATA_PATH, _data_filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASpoNmxKSQjI"
      },
      "source": [
        "Take a quick look at the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eSz28UDSnlG"
      },
      "outputs": [],
      "source": [
        "!head {_data_filepath}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH6gizcpSwWV"
      },
      "source": [
        "## Create a pipeline\n",
        "\n",
        "TFX pipelines are defined using Python APIs. We will define a pipeline which consists of following three components.\n",
        "- CsvExampleGen: Reads in data files and convert them to TFX internal format for further processing. There are multiple [ExampleGen](https://www.tensorflow.org/tfx/guide/examplegen)s for various formats. In this tutorial, we will use CsvExampleGen which takes CSV file input.\n",
        "- Trainer: Trains a ML model. [Trainer component](https://www.tensorflow.org/tfx/guide/trainer) requires a model definition code from users. You can use TensorFlow APIs to specify how to train a model and export it in a saved_model format.\n",
        "- Pusher: Copies the trained model outside of the TFX pipeline. [Pusher component](https://www.tensorflow.org/tfx/guide/pusher) can be thought of an deployment process of the trained ML model.\n",
        "\n",
        "Before actually define the pipeline, we need to write a model code for the Trainer component first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOjDv93eS5xV"
      },
      "source": [
        "### Write model code.\n",
        "\n",
        "We will create a simple DNN model for classification using TensorFlow Keras API. This model code will be saved to a separate file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aES7Hv5QTDK3"
      },
      "outputs": [],
      "source": [
        "_trainer_module_file = 'penguin_trainer.py'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gnc67uQNTDfW"
      },
      "outputs": [],
      "source": [
        "%%writefile {_trainer_module_file}\n",
        "\n",
        "from typing import List, Text\n",
        "import absl\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow_transform.tf_metadata import schema_utils\n",
        "\n",
        "from tfx.components.trainer.executor import TrainerFnArgs\n",
        "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
        "from tfx_bsl.tfxio import dataset_options\n",
        "from tensorflow_metadata.proto.v0 import schema_pb2\n",
        "\n",
        "_FEATURE_KEYS = [\n",
        "    'culmen_length_mm', 'culmen_depth_mm', 'flipper_length_mm', 'body_mass_g'\n",
        "]\n",
        "_LABEL_KEY = 'species'\n",
        "\n",
        "# The Penguin dataset has 342 records, and is divided into train and eval\n",
        "# splits in a 2:1 ratio.\n",
        "_TRAIN_DATA_SIZE = 228\n",
        "_EVAL_DATA_SIZE = 114\n",
        "_TRAIN_BATCH_SIZE = 20\n",
        "_EVAL_BATCH_SIZE = 10\n",
        "\n",
        "_FEATURE_SPEC = {\n",
        "    'culmen_length_mm': tf.io.FixedLenFeature(shape=[1], dtype=tf.float32),\n",
        "    'culmen_depth_mm': tf.io.FixedLenFeature(shape=[1], dtype=tf.float32),\n",
        "    'flipper_length_mm': tf.io.FixedLenFeature(shape=[1], dtype=tf.float32),\n",
        "    'body_mass_g': tf.io.FixedLenFeature(shape=[1], dtype=tf.float32),\n",
        "    'species': tf.io.FixedLenFeature(shape=[1], dtype=tf.int64)\n",
        "}\n",
        "\n",
        "\n",
        "def _get_serve_tf_examples_fn(model, feature_spec):\n",
        "  \"\"\"Returns a function that parses a serialized tf.Example.\"\"\"\n",
        "\n",
        "  @tf.function\n",
        "  def serve_tf_examples_fn(serialized_tf_examples):\n",
        "    \"\"\"Returns the output to be used in the serving signature.\"\"\"\n",
        "    parsed_features = tf.io.parse_example(serialized_tf_examples, feature_spec)\n",
        "\n",
        "    return model(parsed_features)\n",
        "\n",
        "  return serve_tf_examples_fn\n",
        "\n",
        "\n",
        "def _input_fn(file_pattern: List[Text],\n",
        "              data_accessor: DataAccessor,\n",
        "              schema: schema_pb2.Schema,\n",
        "              batch_size: int = 200) -\u003e tf.data.Dataset:\n",
        "  \"\"\"Generates features and label for tuning/training.\n",
        "\n",
        "  Args:\n",
        "    file_pattern: List of paths or patterns of input tfrecord files.\n",
        "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
        "    schema: schema of the input data.\n",
        "    batch_size: representing the number of consecutive elements of returned\n",
        "      dataset to combine in a single batch\n",
        "\n",
        "  Returns:\n",
        "    A dataset that contains (features, indices) tuple where features is a\n",
        "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
        "  \"\"\"\n",
        "  return data_accessor.tf_dataset_factory(\n",
        "      file_pattern,\n",
        "      dataset_options.TensorFlowDatasetOptions(\n",
        "          batch_size=batch_size, label_key=_LABEL_KEY),\n",
        "      schema=schema)\n",
        "\n",
        "\n",
        "def _build_keras_model() -\u003e tf.keras.Model:\n",
        "  \"\"\"Creates a DNN Keras model for classifying penguin data.\n",
        "\n",
        "  Returns:\n",
        "    A Keras Model.\n",
        "  \"\"\"\n",
        "  # The model below is built with Functional API, please refer to\n",
        "  # https://www.tensorflow.org/guide/keras/overview for all API options.\n",
        "  inputs = [keras.layers.Input(shape=(1,), name=f) for f in _FEATURE_KEYS]\n",
        "  d = keras.layers.concatenate(inputs)\n",
        "  for _ in range(2):\n",
        "    d = keras.layers.Dense(8, activation='relu')(d)\n",
        "  outputs = keras.layers.Dense(3, activation='softmax')(d)\n",
        "\n",
        "  model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "  model.compile(\n",
        "      optimizer=keras.optimizers.Adam(1e-2),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "\n",
        "  model.summary(print_fn=absl.logging.info)\n",
        "  return model\n",
        "\n",
        "\n",
        "# TFX Trainer will call this function.\n",
        "def run_fn(fn_args: TrainerFnArgs):\n",
        "  \"\"\"Train the model based on given args.\n",
        "\n",
        "  Args:\n",
        "    fn_args: Holds args used to train the model as name/value pairs.\n",
        "  \"\"\"\n",
        "\n",
        "  # This schema is usually either an output of SchemaGen or a manually-curated\n",
        "  # version provided by pipeline author. A schema can also derived from TFT\n",
        "  # graph if a Transform component is used. In the case when either is missing,\n",
        "  # `schema_from_feature_spec` could be used to generate schema from very simple\n",
        "  # feature_spec, but the schema returned would be very primitive.\n",
        "  schema = schema_utils.schema_from_feature_spec(_FEATURE_SPEC)\n",
        "\n",
        "  train_dataset = _input_fn(\n",
        "      fn_args.train_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_TRAIN_BATCH_SIZE)\n",
        "  eval_dataset = _input_fn(\n",
        "      fn_args.eval_files,\n",
        "      fn_args.data_accessor,\n",
        "      schema,\n",
        "      batch_size=_EVAL_BATCH_SIZE)\n",
        "\n",
        "  mirrored_strategy = tf.distribute.MirroredStrategy()\n",
        "  with mirrored_strategy.scope():\n",
        "    model = _build_keras_model()\n",
        "\n",
        "  steps_per_epoch = _TRAIN_DATA_SIZE // _TRAIN_BATCH_SIZE\n",
        "\n",
        "  # Write logs to path\n",
        "  tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
        "      log_dir=fn_args.model_run_dir, update_freq='batch')\n",
        "\n",
        "  model.fit(\n",
        "      train_dataset,\n",
        "      epochs=fn_args.train_steps // steps_per_epoch,\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      validation_data=eval_dataset,\n",
        "      validation_steps=fn_args.eval_steps,\n",
        "      callbacks=[tensorboard_callback])\n",
        "\n",
        "  signatures = {\n",
        "      'serving_default':\n",
        "          _get_serve_tf_examples_fn(model, _FEATURE_SPEC).get_concrete_function(\n",
        "              tf.TensorSpec(shape=[None], dtype=tf.string, name='examples')),\n",
        "  }\n",
        "  model.save(fn_args.serving_model_dir, save_format='tf', signatures=signatures)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3OkNz3gTLwM"
      },
      "source": [
        "### Write a pipeline definition\n",
        "\n",
        "We define a function to create a TFX pipeline. A `Pipeline` object represents a TFX pipeline which can be run using one of pipeline orchestration system that TFX supports."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M49yYVNBTPd4"
      },
      "outputs": [],
      "source": [
        "from typing import List, Text\n",
        "\n",
        "from tfx.components import CsvExampleGen\n",
        "from tfx.components import Pusher\n",
        "from tfx.components import Trainer\n",
        "from tfx.components.trainer.executor import GenericExecutor\n",
        "from tfx.dsl.components.base import executor_spec\n",
        "from tfx.orchestration import metadata\n",
        "from tfx.orchestration import pipeline\n",
        "from tfx.proto import pusher_pb2\n",
        "from tfx.proto import trainer_pb2\n",
        "from tfx.utils.dsl_utils import external_input\n",
        "\n",
        "def _create_pipeline(pipeline_name: Text, pipeline_root: Text, data_root: Text,\n",
        "                     module_file: Text, serving_model_dir: Text,\n",
        "                     metadata_path: Text) -\u003e pipeline.Pipeline:\n",
        "  \"\"\"Implements the penguin pipeline with TFX.\"\"\"\n",
        "  examples = external_input(data_root)\n",
        "\n",
        "  # Brings data into the pipeline or otherwise joins/converts training data.\n",
        "  example_gen = CsvExampleGen(input=examples)\n",
        "\n",
        "  # Uses user-provided Python function that trains a model.\n",
        "  trainer = Trainer(\n",
        "      module_file=module_file,\n",
        "      custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
        "      examples=example_gen.outputs['examples'],\n",
        "      train_args=trainer_pb2.TrainArgs(num_steps=100),\n",
        "      eval_args=trainer_pb2.EvalArgs(num_steps=5))\n",
        "\n",
        "  # Pushes the model to a file destination if check passed.\n",
        "  pusher = Pusher(\n",
        "      model=trainer.outputs['model'],\n",
        "      push_destination=pusher_pb2.PushDestination(\n",
        "          filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "              base_directory=serving_model_dir)))\n",
        "\n",
        "  components = [\n",
        "      example_gen,\n",
        "      trainer,\n",
        "      pusher,\n",
        "  ]\n",
        "\n",
        "  return pipeline.Pipeline(\n",
        "      pipeline_name=pipeline_name,\n",
        "      pipeline_root=pipeline_root,\n",
        "      components=components,\n",
        "      enable_cache=True,\n",
        "      metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "          metadata_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuB9_bpOTQxV"
      },
      "source": [
        "### Run the pipeline using LocalDagRunner\n",
        "\n",
        "We will use `LocalDagRunner`. LocalDagRunner is a simple local orchestrator without any external dependency and it is good for fast iteration during pipeline development or debugging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK49CqtrTQMW"
      },
      "outputs": [],
      "source": [
        "from tfx.orchestration.local import local_dag_runner\n",
        "local_dag_runner.LocalDagRunner().run(\n",
        "    _create_pipeline(\n",
        "        pipeline_name=_pipeline_name,\n",
        "        pipeline_root=_pipeline_root,\n",
        "        data_root=_data_root,\n",
        "        module_file=_trainer_module_file,\n",
        "        serving_model_dir=_serving_model_dir,\n",
        "        metadata_path=_metadata_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOL78kUufq6d"
      },
      "source": [
        "If the pipeline ran successfully, you should see \"Component Pusher is finished.\" in the log.\n",
        "\n",
        "You can find the generated model at the `serving_model`(the value of `_serving_model_dir` variable) if you didn't change the above code. There should be other artifacts under `pipelines`(the value of `_pipeline_root` variable). If you are on Colab, click folder icon on the left sidebar to browse files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08R8qvweThRf"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "TFX provides many useful components and you can also create your own easily. Please see [TFX components tutorial](https://www.tensorflow.org/tfx/tutorials/tfx/components_keras). If you want to deploy your pipeline into Google Cloud, please see [TFX on Cloud AI Platform](https://www.tensorflow.org/tfx/tutorials/tfx/cloud-ai-platform-pipelines).\n",
        "\n",
        "You can find more resources on https://www.tensorflow.org/tfx/tutorials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMtJk9DaTqeP"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oiTx9KF4O6gt"
      ],
      "name": "Simple TFX Pipeline using Penguin dataset",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
